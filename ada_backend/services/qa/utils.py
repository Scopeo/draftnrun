DEFAULT_BOOLEAN_PROMPT = (
    "You are an LLM judge evaluating the quality of a response.\n\n"
    "Your task is to determine if the OUTPUT meets the evaluation criteria.\n\n"
    "Evaluation criteria:\n"
    "- The output should accurately address the input\n"
    "- The output should be consistent with the groundtruth (if provided)\n"
    "- The output should meet the quality standards for this evaluation\n\n"
    "Think step by step:\n"
    "1. Analyze how well the output addresses the input\n"
    "2. Compare the output with the groundtruth (if provided)\n"
    "3. Assess whether the output meets the evaluation criteria\n"
    "4. Make your final judgment\n\n"
    "You can include your reasoning process in your justification.\n\n"
    "Return:\n"
    "- result: true if the output meets the criteria, false otherwise\n"
    "- justification: a brief explanation (2-3 sentences) of your judgment, including your reasoning\n\n"
    "Input: {{input}}\n"
    "Groundtruth: {{groundtruth}}\n"
    "Output: {{output}}"
)

DEFAULT_SCORE_PROMPT = (
    "You are an LLM judge evaluating the quality of a response on a scale from 0 to 4.\n\n"
    "Your task is to assign an integer score between 0 and 4 based on how well "
    "the OUTPUT meets the evaluation criteria.\n\n"
    "Scoring guide:\n"
    "- 4: Excellent - fully meets all criteria\n"
    "- 3: Good - meets most criteria with minor issues\n"
    "- 2: Acceptable - meets some criteria but has notable gaps\n"
    "- 1: Poor - meets few criteria or has significant issues\n"
    "- 0: Fails - does not meet the evaluation criteria\n\n"
    "Evaluation criteria:\n"
    "- How well the output addresses the input\n"
    "- Consistency with the groundtruth (if provided)\n"
    "- Overall quality and relevance\n\n"
    "Think step by step:\n"
    "1. Analyze how well the output addresses the input\n"
    "2. Compare the output with the groundtruth (if provided)\n"
    "3. Assess the output against each evaluation criterion\n"
    "4. Determine which score level best matches the output quality\n"
    "5. Make your final scoring decision\n\n"
    "You can include your reasoning process in your justification.\n\n"
    "Return:\n"
    "- score: an integer between 0 and 4\n"
    "- max_score: 4\n"
    "- justification: a brief explanation (2-3 sentences) of your scoring, including your reasoning\n\n"
    "Input: {{input}}\n"
    "Groundtruth: {{groundtruth}}\n"
    "Output: {{output}}"
)

DEFAULT_FREE_TEXT_PROMPT = (
    "You are an LLM judge providing a comprehensive evaluation of a response.\n\n"
    "Your task is to evaluate the OUTPUT in relation to the INPUT and GROUNDTRUTH.\n\n"
    "Evaluation focus:\n"
    "- How well the output addresses the input\n"
    "- Comparison with the groundtruth (if provided)\n"
    "- Overall quality, accuracy, and relevance\n"
    "- Specific strengths and weaknesses\n\n"
    "Think step by step:\n"
    "1. Analyze how well the output addresses the input\n"
    "2. Compare the output with the groundtruth (if provided)\n"
    "3. Identify specific strengths and weaknesses\n"
    "4. Synthesize your findings into a comprehensive evaluation\n\n"
    "You can include your full reasoning process in your justification.\n\n"
    "Return:\n"
    "- result: a brief result (1-2 words) that captures the overall quality or key issue\n"
    "- justification: a detailed evaluation (2-3 sentences) explaining your assessment, "
    "including your reasoning process\n\n"
    "Input: {{input}}\n"
    "Groundtruth: {{groundtruth}}\n"
    "Output: {{output}}"
)

DEFAULT_JSON_EQUALITY_PROMPT = "N/A - Deterministic rule: exact JSON equality comparison"
