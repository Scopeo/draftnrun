name: github-ci

on:
  workflow_dispatch:

  pull_request:
    branches:
      - main

concurrency:
  group: ec2-runner-tests
  cancel-in-progress: false

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      migration-or-seed-changed: ${{ steps.filter.outputs.migration-or-seed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Check for migration or seed changes
        uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            migration-or-seed:
              - 'ada_backend/database/**'

  ci-pipeline:
    needs: detect-changes
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      matrix:
        python-version: [3.11]

    env:
      EC2_USER: ec2-user
      EC2_PATH_BASE: /home/ec2-user/pr-tests
      EC2_FOLDER_NAME: pr_

    steps:
      - name: Generate unique ID
        run: |
          RUN_ID=$(uuidgen)
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v2
        with:
          lfs: true

      - name: Set up SSH key
        uses: webfactory/ssh-agent@v0.5.3
        with:
          ssh-private-key: ${{ secrets.DEPLOY_KEY }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Copy files to EC2
        uses: appleboy/scp-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          source: "."
          target: "${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{env.RUN_ID}}"

      - name: Install ci dependencies
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{env.RUN_ID}}
            pip install uv
            uv sync

      - name: Linter
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{env.RUN_ID}}
            uv run ruff check .

      - name: Create credentials.env file
        run: |
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> credentials.env
          echo "COHERE_API_KEY=${{ secrets.COHERE_API_KEY }}" >> credentials.env
          echo "MISTRAL_API_KEY=${{ secrets.MISTRAL_API_KEY }}" >> credentials.env
          echo "GOOGLE_API_KEY=${{ secrets.GOOGLE_API_KEY }}" >> credentials.env
          echo "GOOGLE_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/" >> credentials.env
          echo "CEREBRAS_API_KEY=${{ secrets.CEREBRAS_API_KEY }}" >> credentials.env
          echo "CEREBRAS_BASE_URL=https://api.cerebras.ai/v1" >> credentials.env
          echo "ANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}" >> credentials.env
          echo "ANTHROPIC_BASE_URL=https://api.anthropic.com/v1/messages" >> credentials.env
          echo "SNOWFLAKE_USER=${{ secrets.SNOWFLAKE_USER }}" >> credentials.env
          echo "SNOWFLAKE_PASSWORD=${{ secrets.SNOWFLAKE_PASSWORD }}" >> credentials.env
          echo "SNOWFLAKE_ACCOUNT=${{ secrets.SNOWFLAKE_ACCOUNT }}" >> credentials.env
          echo "QDRANT_CLUSTER_URL=${{ secrets.QDRANT_CLUSTER_URL }}" >> credentials.env
          echo "QDRANT_API_KEY=${{ secrets.QDRANT_API_KEY }}" >> credentials.env
          echo "TAVILY_API_KEY=${{ secrets.TAVILY_API_KEY }}" >> credentials.env
          echo "FERNET_KEY=${{ secrets.FERNET_KEY }}" >> credentials.env
          echo "SUPABASE_PROJECT_URL=${{ secrets.SUPABASE_PROJECT_URL }}" >> credentials.env
          echo "SUPABASE_PROJECT_KEY=${{ secrets.SUPABASE_PROJECT_KEY }}" >> credentials.env
          echo "SUPABASE_SERVICE_ROLE_SECRET_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_SECRET_KEY }}" >> credentials.env
          echo "TEST_USER_EMAIL=${{ secrets.TEST_USER_EMAIL }}" >> credentials.env
          echo "TEST_USER_PASSWORD=${{ secrets.TEST_USER_PASSWORD }}" >> credentials.env
          echo "BACKEND_SECRET_KEY=${{ secrets.BACKEND_SECRET_KEY }}" >> credentials.env
          echo "INGESTION_API_KEY=${{ secrets.INGESTION_API_KEY }}" >> credentials.env
          echo "INGESTION_API_KEY_HASHED=${{ secrets.INGESTION_API_KEY_HASHED }}" >> credentials.env
          echo "ADA_URL=${{ secrets.ADA_URL }}" >> credentials.env
          echo "REDIS_HOST=${{ secrets.REDIS_HOST }}" >> credentials.env
          echo "REDIS_PORT=${{ secrets.REDIS_PORT }}" >> credentials.env
          echo "REDIS_PASSWORD=${{ secrets.REDIS_PASSWORD }}" >> credentials.env
          echo "REDIS_QUEUE_NAME=${{ secrets.REDIS_QUEUE_NAME }}" >> credentials.env
          echo "ADA_DB_DRIVER=postgres" >> credentials.env
          echo "ADA_DB_URL=postgresql://postgres:ada_password@localhost:5432/ada_backend" >> credentials.env
          echo "INGESTION_DB_URL=postgresql://postgres:ada_password@localhost:5432/ada_ingestion" >> credentials.env
          echo "TRACES_DB_URL=postgresql://postgres:ada_password@localhost:5432/ada_traces" >> credentials.env
          echo "ADA_URL=http://localhost:8000" >> credentials.env
          echo "E2B_API_KEY=${{ secrets.E2B_API_KEY }}" >> credentials.env
          echo "S3_ACCESS_KEY_ID=${{ secrets.S3_ACCESS_KEY_ID }}" >> credentials.env
          echo "S3_SECRET_ACCESS_KEY=${{ secrets.S3_SECRET_ACCESS_KEY }}" >> credentials.env
          echo "S3_BUCKET_NAME=${{ secrets.S3_BUCKET_NAME }}" >> credentials.env
          echo "S3_REGION_NAME=eu-west-3" >> credentials.env
          echo "ENABLE_OBSERVABILITY_STACK=false" >> credentials.env
          echo "SEGMENT_API_KEY=${{ secrets.SEGMENT_API_KEY }}" >> credentials.env
          echo "ENV=${{ secrets.ENV }}" >> credentials.env
          echo "PAGE_RESOLUTION_ZOOM=3.0" >> credentials.env
          echo "NUMBER_OF_PAGES_TO_DETECT_DOCUMENT_TYPE=5" >> credentials.env
          echo "LINKUP_API_KEY=${{secrets.LINKUP_API_KEY}}" >> cedentials.env
          echo "PROD_DB_URL=${{ secrets.PROD_DB_URL }}" >> credentials.env
          echo "USE_LLM_FOR_PDF_PARSING=False" >> credentials.env

      - name: Upload credentials.env to EC2
        uses: appleboy/scp-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          source: "credentials.env"
          target: "${{ env.EC2_PATH_BASE }}/${{ env.EC2_FOLDER_NAME }}${{ env.RUN_ID }}"

      - name: Check OS
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            cat /etc/os-release

      - name: Install Docker and Docker Compose
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Installing Docker and Docker Compose on Amazon Linux 2023"

            # Install Docker if not present
            if ! command -v docker &> /dev/null; then
              sudo dnf install -y docker
              sudo systemctl enable --now docker
            else
              echo "===> Docker already installed"
            fi

            # Install Docker Compose v2 if not present
            if ! docker compose version &> /dev/null; then
              echo "===> Installing Docker Compose v2"
              DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}
              mkdir -p $DOCKER_CONFIG/cli-plugins
              curl -SL https://github.com/docker/compose/releases/download/v2.24.6/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose
              chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose
            else
              echo "===> Docker Compose already installed"
            fi

      - name: Start PostgreSQL test container
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}/services

            echo "===> Removing existing ada_postgres container if it exists"
            docker rm -f ada_postgres || true

            echo "===> Removing existing ada_postgres container if it exists"
            docker compose down -v --remove-orphans

            echo "===> Starting postgres container"
            docker compose up -d postgres

            echo "===> Waiting for PostgreSQL to be ready"
            timeout 30s bash -c 'until docker compose exec postgres pg_isready; do sleep 1; done'

      - name: Alembic migration checks
        if: needs.detect-changes.outputs.migration-or-seed-changed == 'true' || github.event_name == 'workflow_dispatch'
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}
            echo "===> Running pytest-alembic suite"
            uv run pytest -q -m alembic tests/alembic

      - name: Restore prod dump and seed
        if: needs.detect-changes.outputs.migration-or-seed-changed == 'true' || github.event_name == 'workflow_dispatch'
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}

            # Source db_utils.sh to use ensure_pg_tools function
            source "./scripts/copy_data/db_utils.sh"
            ensure_pg_tools

            # Load credentials if present (optional)
            if [ -f credentials.env ]; then
              set -a; source credentials.env; set +a
            fi

            # Require PROD_DB_URL to be set on the EC2 host or in credentials.env
            if [ -z "${PROD_DB_URL:-}" ]; then
              echo "ERROR: PROD_DB_URL is not set on the test EC2 instance." >&2
              exit 1
            fi


            # TTL cache for prod dumps
            CACHE_DIR="/home/ec2-user/ci-cache/db-dumps"
            mkdir -p "$CACHE_DIR"
            DUMP_PATH="$CACHE_DIR/prod.latest.dump"
            TTL_MINUTES=1440  # 24 hours

            # Helper function to check dump age and TTL
            check_dump_ttl() {
              local dump_file="$1"
              local dump_name="$2"
              if [ ! -f "$dump_file" ]; then
                echo "===> $dump_name: No cached dump found, will create new one"
                return 1
              fi
              local current_time=$(date +%s)
              local file_time=$(stat -c %Y "$dump_file")
              local age_seconds=$((current_time - file_time))
              local age_minutes=$((age_seconds / 60))
              local remaining_minutes=$((TTL_MINUTES - age_minutes))
              local age_hours=$((age_minutes / 60))
              local remaining_hours=$((remaining_minutes / 60))
              if [ $remaining_minutes -le 0 ]; then
                echo "===> $dump_name: Cache expired (age: ${age_hours}h), will refresh"
                return 1
              else
                echo "===> $dump_name: Cache age: ${age_hours}h, TTL remaining: ${remaining_hours}h (${remaining_minutes}m)"
                return 0
              fi
            }

            # Dump/refresh main DB
            echo "===> Checking main DB dump cache at $DUMP_PATH (TTL 24h)"
            if ! check_dump_ttl "$DUMP_PATH" "Main DB"; then
              echo "===> Refreshing prod dump"
              pg_dump -Fc "$PROD_DB_URL" -f "$DUMP_PATH"
              echo "===> Main DB dump refreshed successfully"
            else
              echo "===> Using cached prod dump"
            fi

            # Validate dump using shared script
            bash "./scripts/copy_data/validate_dump.sh" "$DUMP_PATH"

            echo "===> Copying dumps into postgres container"
            cd services
            CONTAINER_ID=$(docker compose ps -q postgres)
            docker cp "$DUMP_PATH" "$CONTAINER_ID:/tmp/prod.dump"

            # Restore main DB
            echo "===> Dropping and creating target database (ada_backend)"
            docker compose exec -T postgres bash -lc 'dropdb -U postgres ada_backend || true && createdb -U postgres ada_backend'

            echo "===> Restoring main dump"
            docker compose exec -T postgres pg_restore -U postgres -d ada_backend \
              --clean --if-exists --no-owner --no-privileges -Fc /tmp/prod.dump

            echo "===> Running database migrations"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}
            make db-upgrade
            make db-seed

      - name: Setup basic database (when no migration/seed changes)
        if: needs.detect-changes.outputs.migration-or-seed-changed != 'true' && github.event_name != 'workflow_dispatch'
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}
            
            echo "===> Setting up basic database for tests"
            make db-upgrade
            make db-seed
            echo "===> Seeding projects database"
            uv run python -m ada_backend.database.seed_project_db


      - name: Tests and coverage
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            echo "===> Changing directory to ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            cd ${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{env.RUN_ID}}
            echo "===> Running tests for RUN_ID: ${{ env.RUN_ID }}"
            uv run coverage run -m pytest --ignore=tests/external_api_calls --ignore=tests/qdrant -m "not pdf_integration" --durations=10
            uv run coverage report -i
            uv run coverage html -i
            uv run coverage xml -i

      - name: Cleanup EC2 folder and Docker resources
        if: always()
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_TEST }}
          username: ${{ env.EC2_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            EC2_TEST_DIR="${{ env.EC2_PATH_BASE }}/${{env.EC2_FOLDER_NAME}}${{ env.RUN_ID }}"
            echo "===> Changing directory to $EC2_TEST_DIR for cleanup"
            cd "$EC2_TEST_DIR" || { echo "Directory not found, skipping Docker cleanup."; }

            # Ensure that docker-compose.yml exists before running docker compose down
            if [ -f docker-compose.yml ]; then
              echo "===> Stopping and removing Docker Compose services and volumes for this run."
              docker compose down -v --remove-orphans || true
            else
              echo "===> docker-compose.yml not found, skipping Docker Compose cleanup."
            fi

            echo "===> Cleaning up project directory: $EC2_TEST_DIR"
            rm -rf "$EC2_TEST_DIR"
