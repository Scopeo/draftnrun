name: Sync Prod DB to Staging

on:
  workflow_dispatch:
    inputs:
      force_sync:
        description: "Force full data + Qdrant sync even if TTL cache is valid"
        required: false
        default: false
        type: boolean

jobs:
  sync-database:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run sync as Kubernetes Job
        id: sync
        continue-on-error: true
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_K8S_STAGING }}
          username: ec2-user
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          command_timeout: 15m
          script: |
            set -e
            NS="ada-staging"
            STATE_FILE="/tmp/ada-sync-status-${{ github.run_id }}.txt"
            JOB_NAME="data-sync-${{ github.run_id }}"

            # Get the current image from the deployment
            IMAGE=$(kubectl get deployment/ada-api -n ${NS} -o jsonpath='{.spec.template.spec.containers[0].image}')

            echo "üîç Running sync as Kubernetes Job using image: ${IMAGE}"
            
            # Build force sync flag
            FORCE_SYNC_FLAG=""
            if [ "${{ inputs.force_sync }}" = "true" ]; then
              FORCE_SYNC_FLAG="--force-sync"
            fi

            # Create Job to run sync (code already exists in the image)
            # Note: Setting PATH to include .venv for uv and Python packages
            kubectl create job ${JOB_NAME} -n ${NS} \
              --image=${IMAGE} \
              -- bash -c "\
                export PATH=/app/.venv/bin:\$PATH && \
                export PYTHONPATH=/app && \
                cd /app && \
                ./scripts/copy_data/sync_data.sh \
                  --source-db-url '${{ secrets.PROD_DB_URL }}' \
                  --source-ingestion-db-url '${{ secrets.PROD_INGESTION_DB_URL }}' \
                  --target-db-url '${{ secrets.STAGING_DB_URL }}' \
                  --target-ingestion-db-url '${{ secrets.STAGING_INGESTION_DB_URL }}' \
                  --source-qdrant-url '${{ secrets.PROD_QDRANT_CLUSTER_URL }}' \
                  --source-qdrant-key '${{ secrets.PROD_QDRANT_API_KEY }}' \
                  --target-qdrant-url '${{ secrets.STAGING_QDRANT_CLUSTER_URL }}' \
                  --target-qdrant-key '${{ secrets.STAGING_QDRANT_API_KEY }}' \
                  ${FORCE_SYNC_FLAG} \
              "

            # Wait for job to complete (or fail)
            echo "‚è≥ Waiting for sync job to complete..."
            if kubectl wait --for=condition=complete job/${JOB_NAME} -n ${NS} --timeout=15m 2>/dev/null; then
              # Job completed successfully - check logs to see if it was skipped
              echo "üìã Checking job logs..."
              LOGS=$(kubectl logs job/${JOB_NAME} -n ${NS})
              echo "$LOGS"
              
              if echo "$LOGS" | grep -qi "skip\|cache.*valid\|already.*synced"; then
                echo "‚è≠Ô∏è Sync not needed - TTL cache is valid"
                echo "SKIPPED" > "$STATE_FILE"
                kubectl delete job ${JOB_NAME} -n ${NS}
                exit 0
              else
                echo "‚úÖ Sync completed successfully"
                echo "SYNCED" > "$STATE_FILE"
                kubectl delete job ${JOB_NAME} -n ${NS}
                exit 0
              fi
            else
              # Job failed
              echo "‚ùå Sync job failed"
              echo "üìã Job logs:"
              kubectl logs job/${JOB_NAME} -n ${NS} || true
              echo "FAILED" > "$STATE_FILE"
              kubectl delete job ${JOB_NAME} -n ${NS} || true
              exit 1
            fi

      - name: Save replica counts and scale down pods
        if: success()
        id: scale_down
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_K8S_STAGING }}
          username: ec2-user
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            NS="ada-staging"
            STATE_FILE="/tmp/ada-sync-status-${{ github.run_id }}.txt"
            REPLICA_FILE="/tmp/ada-sync-replicas-${{ github.run_id }}.txt"
            
            # Check if sync was actually performed
            if [ -f "$STATE_FILE" ]; then
              SYNC_STATUS=$(cat "$STATE_FILE")
              if [ "$SYNC_STATUS" = "SKIPPED" ]; then
                echo "‚è≠Ô∏è Skipping scale down - sync was not needed"
                exit 0
              fi
            fi
            
            # Save current replica counts
            echo "üíæ Saving current replica counts..."
            kubectl get deployment -n ${NS} -o json | \
              jq -r '.items[] | "\(.metadata.name)=\(.spec.replicas)"' > "$REPLICA_FILE"
            
            cat "$REPLICA_FILE"
            
            # Scale down with error handling
            echo "‚¨áÔ∏è Scaling down all deployments..."
            if ! kubectl scale deployment --all -n ${NS} --replicas=0; then
              echo "‚ùå Failed to scale down deployments"
              rm -f "$REPLICA_FILE"
              exit 1
            fi
            
            kubectl wait --for=delete pod -l app -n ${NS} --timeout=300s || {
              echo "‚ö†Ô∏è Warning: Some pods did not terminate within timeout"
            }

      - name: Run migrations and seed as Jobs
        if: success()
        continue-on-error: true
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_K8S_STAGING }}
          username: ec2-user
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          command_timeout: 15m
          script: |
            set -e
            NS="ada-staging"
            STATE_FILE="/tmp/ada-sync-status-${{ github.run_id }}.txt"
            
            # Check if sync was actually performed
            if [ -f "$STATE_FILE" ]; then
              SYNC_STATUS=$(cat "$STATE_FILE")
              if [ "$SYNC_STATUS" = "SKIPPED" ]; then
                echo "‚è≠Ô∏è Skipping migrations - sync was not performed"
                exit 0
              fi
            fi

            echo "üóÑÔ∏è Running migrations as Kubernetes Job..."
            kubectl create job db-migrate -n ${NS} \
              --from=deployment/ada-api \
              -- alembic -c ada_backend/database/alembic.ini upgrade head

            kubectl wait --for=condition=complete job/db-migrate -n ${NS} --timeout=300s
            kubectl delete job db-migrate -n ${NS}

            echo "üå± Running seed as Kubernetes Job..."
            kubectl create job db-seed -n ${NS} \
              --from=deployment/ada-api \
              -- python -m ada_backend.database.seed_db

            kubectl wait --for=condition=complete job/db-seed -n ${NS} --timeout=300s
            kubectl delete job db-seed -n ${NS}

            echo "‚úÖ Database migrations and seed complete!"

      - name: Scale up pods to original replica counts
        if: always()
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.EC2_HOST_K8S_STAGING }}
          username: ec2-user
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: 22
          script: |
            set -e
            NS="ada-staging"
            STATE_FILE="/tmp/ada-sync-status-${{ github.run_id }}.txt"
            REPLICA_FILE="/tmp/ada-sync-replicas-${{ github.run_id }}.txt"
            
            # Check if sync was skipped (no scale down happened)
            if [ -f "$STATE_FILE" ]; then
              SYNC_STATUS=$(cat "$STATE_FILE")
              if [ "$SYNC_STATUS" = "SKIPPED" ]; then
                echo "‚è≠Ô∏è No scale up needed - sync was skipped, pods never scaled down"
                rm -f "$STATE_FILE" /tmp/sync-output.log
                exit 0
              fi
            fi
            
            # Scale up was needed (sync happened)
            if [ ! -f "$REPLICA_FILE" ]; then
              echo "‚ö†Ô∏è Warning: Replica state file not found, using defaults"
              kubectl scale deployment/ada-api -n ${NS} --replicas=1 || true
              kubectl scale deployment/ada-ingestion-worker -n ${NS} --replicas=1 || true
              kubectl scale deployment/ada-webhook-worker -n ${NS} --replicas=1 || true
              kubectl scale deployment/ada-scheduler -n ${NS} --replicas=1 || true
            else
              echo "‚¨ÜÔ∏è Scaling back to original replica counts..."
              while IFS='=' read -r deployment replicas; do
                echo "Scaling ${deployment} to ${replicas} replicas"
                kubectl scale deployment/${deployment} -n ${NS} --replicas=${replicas} || true
              done < "$REPLICA_FILE"
              
              # Cleanup state files
              rm -f "$REPLICA_FILE" "$STATE_FILE" /tmp/sync-output.log
            fi
            
            echo "‚úÖ Pods scaled back up!"
